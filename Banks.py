# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kPbwveRFi9dBic81w5T-sWrOpdfJyfLL

### Депозиты 1 месяц
"""

import matplotlib.pyplot as plt
import pandas as pd

df_dep_1m = pd.read_csv('/content/sample_data/Itog_ALL_deposits_1_month (1).csv')
df_dep_1m.head(5)

data_info = df_dep_1m.info()
df_dep_1m.isnull().sum()

"""#### Депозиты 1 месяц кластеры на основе макс ставки и мин депозит"""

from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
from sklearn.metrics import silhouette_score
import numpy as np

# Выбор числовых переменных для кластеризации
numerical_data = df_dep_1m[['Максимальная ставка',
                       'Минимальный депозит']]

# Масштабирование данных
scaler = StandardScaler()
scaled_data = scaler.fit_transform(numerical_data)

# Определение оптимального количества кластеров
range_clusters = range(2, 11)
inertia = []
silhouette = []

for n_clusters in range_clusters:
    kmeans = KMeans(n_clusters=n_clusters, random_state=42)
    kmeans.fit(scaled_data)
    inertia.append(kmeans.inertia_)
    silhouette.append(silhouette_score(scaled_data, kmeans.labels_))

# Визуализация метода локтя
plt.figure(figsize=(15, 5))
plt.subplot(1, 2, 1)
plt.plot(range_clusters, inertia, marker='o')
plt.xlabel('Number of clusters')
plt.ylabel('Inertia')
plt.title('Elbow Method')

# Визуализация метода силуэта
plt.subplot(1, 2, 2)
plt.plot(range_clusters, silhouette, marker='o')
plt.xlabel('Number of clusters')
plt.ylabel('Silhouette Score')
plt.title('Silhouette Method')

plt.tight_layout()
plt.show()

from sklearn.preprocessing import StandardScaler
numerical_data = df_dep_1m[['Максимальная ставка',
                       'Минимальный депозит']]
# Создание экземпляра StandardScaler
scaler = StandardScaler()

# Применение масштабирования
df_scaled = scaler.fit_transform(numerical_data)

# Преобразование обратно в DataFrame для удобства работы
df_scaled = pd.DataFrame(df_scaled, columns=numerical_data.columns)

# Просмотр масштабированных данных
print(df_scaled)

# Применение K-means кластеризации с 3 кластерами
kmeans = KMeans(n_clusters=3, random_state=42)
kmeans.fit(df_scaled)

# Добавление меток кластеров к исходным данным
df_dep_1m['Кластер'] = kmeans.labels_

# Просмотр данных с метками кластеров
print(df_dep_1m)

import matplotlib.pyplot as plt
from sklearn.decomposition import PCA

# Применение PCA для уменьшения размерности до 2D
pca = PCA(n_components=2)
principalComponents = pca.fit_transform(df_scaled)

# Создание DataFrame для результатов PCA
pca_df = pd.DataFrame(data = principalComponents, columns = ['PC1', 'PC2'])

# Добавление меток кластеров
pca_df['Cluster'] = kmeans.labels_

# Визуализация результатов
plt.figure(figsize=(8, 6))
colors = ['r', 'g', 'b']
for color, cluster in zip(colors, range(3)):
    temp_df = pca_df[pca_df['Cluster'] == cluster]
    plt.scatter(temp_df['PC1'], temp_df['PC2'], color=color, label=f'Cluster {cluster + 1}')

plt.title('2D PCA of KMeans Clusters')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.legend()
plt.show()

# Перебираем уникальные метки кластеров
for cluster_label in pca_df['Cluster'].unique():
    # Фильтруем DataFrame, оставляя только строки с текущей меткой кластера
    cluster_elements = pca_df[pca_df['Cluster'] == cluster_label]

    # Выводим элементы текущего кластера
    print(f'Cluster {cluster_label} elements:')
    print(cluster_elements)

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import AgglomerativeClustering
from scipy.cluster.hierarchy import dendrogram

# Загрузка данных (замените data.csv на свой файл данных)
data = pd.read_csv('/content/sample_data/Itog_ALL_deposits_1_month (1).csv')

# Выбор числовых признаков для кластеризации
numerical_data = data[['Максимальная ставка', 'Минимальная ставка', 'Минимальный депозит']]

# Масштабирование данных
scaler = StandardScaler()
scaled_data = scaler.fit_transform(numerical_data)

# Выполнение иерархической кластеризации
agg_clustering = AgglomerativeClustering(n_clusters=3)  # Здесь указано количество кластеров
agg_clusters = agg_clustering.fit_predict(scaled_data)

# Визуализация дендрограммы (иерархии)
from scipy.cluster.hierarchy import linkage, dendrogram
linkage_matrix = linkage(scaled_data, method='ward')  # Метод 'ward' используется для объединения кластеров
dendrogram(linkage_matrix, labels=data.index, orientation='top')
plt.title('Dendrogram')
plt.xlabel('Sample Index')
plt.ylabel('Distance')
plt.show()

# Вывод результатов иерархической кластеризации
result_df = pd.DataFrame({'Cluster': agg_clusters}, index=data.index)
print(result_df)

# Вывод данных для кластера с меткой 0
cluster_label = 0  # Замените на номер кластера, который вас интересует
cluster_data = data[agg_clusters == cluster_label]

# Вывод данных для кластера
print(cluster_data)

# Вывод данных для кластера с меткой 1
cluster_label = 1  # Замените на номер кластера, который вас интересует
cluster_data = data[agg_clusters == cluster_label]

# Вывод данных для кластера
print(cluster_data)

# Вывод данных для кластера с меткой 2
cluster_label = 2  # Замените на номер кластера, который вас интересует
cluster_data = data[agg_clusters == cluster_label]

# Вывод данных для кластера
print(cluster_data)

"""## Кредиты 1 год"""

import pandas as pd
df = pd.read_csv('/content/sample_data/ALL_loans_1_year (1).csv')
df.head(5)

data_info = df.info()
df.isnull().sum()



"""### Кредиты 1 год кластеры на основе мин процент и макс суммы


"""

from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
from sklearn.metrics import silhouette_score
import numpy as np

# Выбор числовых переменных для кластеризации
numerical_data = df[['Минимальный процент', 'Максимальная сумма']]

# Масштабирование данны
# scaler = StandardScaler()
# scaled_data = scaler.fit_transform(numerical_data)

# Определение оптимального количества кластеров
range_clusters = range(2, 11)
inertia = []
silhouette = []

for n_clusters in range_clusters:
    kmeans = KMeans(n_clusters=n_clusters, random_state=42)
    kmeans.fit(scaled_data)
    inertia.append(kmeans.inertia_)
    silhouette.append(silhouette_score(scaled_data, kmeans.labels_))

# Визуализация метода локтя
plt.figure(figsize=(15, 5))
plt.subplot(1, 2, 1)
plt.plot(range_clusters, inertia, marker='o')
plt.xlabel('Number of clusters')
plt.ylabel('Inertia')
plt.title('Elbow Method')

# Визуализация метода силуэта
plt.subplot(1, 2, 2)
plt.plot(range_clusters, silhouette, marker='o')
plt.xlabel('Number of clusters')
plt.ylabel('Silhouette Score')
plt.title('Silhouette Method')

plt.tight_layout()
plt.show()

"""Следовательно количество кластеров = 5

"""

from sklearn.preprocessing import StandardScaler

# Создание экземпляра StandardScaler
scaler = StandardScaler()

# Применение масштабирования
df_scaled = scaler.fit_transform(numerical_data)

# Преобразование обратно в DataFrame для удобства работы
df_scaled = pd.DataFrame(df_scaled, columns=numerical_data.columns)

# Просмотр масштабированных данных
print(df_scaled)

# Применение K-means кластеризации с 5 кластерами
kmeans = KMeans(n_clusters=5, random_state=42)
kmeans.fit(df_scaled)

# Добавление меток кластеров к исходным данным
df['Кластер'] = kmeans.labels_

# Просмотр данных с метками кластеров
print(df)

import matplotlib.pyplot as plt
from sklearn.decomposition import PCA

# Применение PCA для уменьшения размерности до 2D
pca = PCA(n_components=2)
principalComponents = pca.fit_transform(df_scaled)

# Создание DataFrame для результатов PCA
pca_df = pd.DataFrame(data = principalComponents, columns = ['PC1', 'PC2'])

# Добавление меток кластеров
pca_df['Cluster'] = kmeans.labels_

# Визуализация результатов
plt.figure(figsize=(8, 6))
colors = ['r', 'g', 'b']
for color, cluster in zip(colors, range(5)):
    temp_df = pca_df[pca_df['Cluster'] == cluster]
    plt.scatter(temp_df['PC1'], temp_df['PC2'], color=color, label=f'Cluster {cluster + 1}')

plt.title('2D PCA of KMeans Clusters')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.legend()
plt.show()

# Перебираем уникальные метки кластеров
for cluster_label in pca_df['Cluster'].unique():
    # Фильтруем DataFrame, оставляя только строки с текущей меткой кластера
    cluster_elements = pca_df[pca_df['Cluster'] == cluster_label]

    # Выводим элементы текущего кластера
    print(f'Cluster {cluster_label} elements:')
    print(cluster_elements)

# Вывод данных для кластера с меткой 1
cluster_label = 1  # Замените на номер кластера, который вас интересует
cluster_data = data[agg_clusters == cluster_label]

# Вывод данных для кластера
print(cluster_data)

# Вывод данных для кластера с меткой 2
cluster_label = 2  # Замените на номер кластера, который вас интересует
cluster_data = data[agg_clusters == cluster_label]

# Вывод данных для кластера
print(cluster_data)

"""### Кредиты 1 год кластеры на основе мин возраст и мин ставки


"""

from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
from sklearn.metrics import silhouette_score
import numpy as np

# Выбор числовых переменных для кластеризации
numerical_data = df[['Минимальный возраст', 'Минимальный процент']]

# Масштабирование данны
scaler = StandardScaler()
scaled_data = scaler.fit_transform(numerical_data)

# Определение оптимального количества кластеров
range_clusters = range(2, 11)
inertia = []
silhouette = []

for n_clusters in range_clusters:
    kmeans = KMeans(n_clusters=n_clusters, random_state=42)
    kmeans.fit(scaled_data)
    inertia.append(kmeans.inertia_)
    silhouette.append(silhouette_score(scaled_data, kmeans.labels_))

# Визуализация метода локтя
plt.figure(figsize=(15, 5))
plt.subplot(1, 2, 1)
plt.plot(range_clusters, inertia, marker='o')
plt.xlabel('Number of clusters')
plt.ylabel('Inertia')
plt.title('Elbow Method')

# Визуализация метода силуэта
plt.subplot(1, 2, 2)
plt.plot(range_clusters, silhouette, marker='o')
plt.xlabel('Number of clusters')
plt.ylabel('Silhouette Score')
plt.title('Silhouette Method')

plt.tight_layout()
plt.show()

# Применение K-means кластеризации с 2 кластерами
kmeans = KMeans(n_clusters=3, random_state=42)
kmeans.fit(numerical_data)

# Добавление меток кластеров к исходным данным
df['Кластер'] = kmeans.labels_

# Просмотр данных с метками кластеров
print(df)

import matplotlib.pyplot as plt
from sklearn.decomposition import PCA

# Применение PCA для уменьшения размерности до 2D
pca = PCA(n_components=2)
principalComponents = pca.fit_transform(df_scaled)

# Создание DataFrame для результатов PCA
pca_df = pd.DataFrame(data = principalComponents, columns = ['PC1', 'PC2'])

# Добавление меток кластеров
pca_df['Cluster'] = kmeans.labels_

# Визуализация результатов
plt.figure(figsize=(8, 6))
colors = ['r', 'g', 'b']
for color, cluster in zip(colors, range(2)):
    temp_df = pca_df[pca_df['Cluster'] == cluster]
    plt.scatter(temp_df['PC1'], temp_df['PC2'], color=color, label=f'Cluster {cluster + 1}')

plt.title('2D PCA of KMeans Clusters')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.legend()
plt.show()

# Вывод данных для кластера с меткой 1
cluster_label = 1  # Замените на номер кластера, который вас интересует
cluster_data = data[agg_clusters == cluster_label]

# Вывод данных для кластера
print(cluster_data)

# Вывод данных для кластера с меткой 2
cluster_label = 2  # Замените на номер кластера, который вас интересует
cluster_data = data[agg_clusters == cluster_label]

# Вывод данных для кластера
print(cluster_data)